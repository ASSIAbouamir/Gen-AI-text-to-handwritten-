{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Projet Text to Handwriting â€“ PrÃ©paration du Dataset IAM et ImplÃ©mentation d'un cGAN\n",
                "Ce notebook combine la prÃ©paration des donnÃ©es du dataset IAM et l'implÃ©mentation d'un Conditional GAN (cGAN) pour gÃ©nÃ©rer des images manuscrites Ã  partir de texte."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. PrÃ©paration des donnÃ©es"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Dossier trouvÃ© : IAM_dataset\n",
                        "ðŸ“ Dossier des images : IAM_dataset\\lines\n",
                        "Nombre de batches dans train_loader : 249\n",
                        "Nombre de batches dans val_loader : 54\n",
                        "Nombre de batches dans test_loader : 54\n",
                        "Batch images shape : torch.Size([32, 1, 64, 256])\n",
                        "Batch texts encoded shape : torch.Size([32, 50, 95])\n",
                        "Exemple de texte : she was a lady who , like her Uncle Charles ,\n"
                    ]
                }
            ],
            "source": [
                "# Importer les bibliothÃ¨ques nÃ©cessaires\n",
                "import os\n",
                "import pandas as pd\n",
                "import cv2\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "from PIL import Image\n",
                "import torch\n",
                "from torchvision import transforms\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import string\n",
                "\n",
                "# DÃ©finir le chemin du dossier IAM\n",
                "data_dir = \"IAM_dataset\"\n",
                "lines_file = os.path.join(data_dir, \"ascii\", \"lines.txt\")\n",
                "lines_img_dir = os.path.join(data_dir, \"lines\")  # Dossier des images\n",
                "\n",
                "# VÃ©rifier si les dossiers existent\n",
                "if not os.path.exists(data_dir):\n",
                "    print(\"âŒ Erreur : Le dossier IAM_dataset n'existe pas.\")\n",
                "elif not os.path.exists(lines_img_dir):\n",
                "    print(\"âŒ Erreur : Le dossier lines n'existe pas dans IAM_dataset.\")\n",
                "else:\n",
                "    print(f\"âœ… Dossier trouvÃ© : {data_dir}\")\n",
                "    print(f\"ðŸ“ Dossier des images : {lines_img_dir}\")\n",
                "\n",
                "# Lecture et parsing du fichier lines.txt\n",
                "lines_data = []\n",
                "with open(lines_file, 'r', encoding='utf-8') as f:\n",
                "    for line in f:\n",
                "        if line.startswith('#') or not line.strip():\n",
                "            continue\n",
                "        parts = line.strip().split()\n",
                "        if len(parts) >= 9:\n",
                "            img_id = parts[0]\n",
                "            status = parts[1]\n",
                "            if status == 'ok':\n",
                "                text = ' '.join(parts[8:]).replace('|', ' ')\n",
                "                img_base = img_id.split('-')[0]\n",
                "                sub_dir = '-'.join(img_id.split('-')[:2])\n",
                "                img_name = f\"{img_id}.png\"\n",
                "                img_path = os.path.join(lines_img_dir, img_base, sub_dir, img_name)\n",
                "                if os.path.exists(img_path):\n",
                "                    lines_data.append((img_path, text))\n",
                "                else:\n",
                "                    print(f\"âš ï¸ Image non trouvÃ©e : {img_path}\")\n",
                "\n",
                "# CrÃ©er un DataFrame\n",
                "df_lines = pd.DataFrame(lines_data, columns=['image_path', 'text'])\n",
                "\n",
                "# Diviser en ensembles d'entraÃ®nement, validation et test\n",
                "train_df, temp_df = train_test_split(df_lines, test_size=0.3, random_state=42)\n",
                "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
                "\n",
                "# Encodage du texte\n",
                "vocab = string.ascii_letters + string.digits + string.punctuation + \" \"\n",
                "vocab_size = len(vocab)\n",
                "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
                "\n",
                "def text_to_one_hot(text, max_len=50):\n",
                "    one_hot = torch.zeros(max_len, vocab_size)\n",
                "    for i, char in enumerate(text[:max_len]):\n",
                "        if char in char_to_idx:\n",
                "            one_hot[i, char_to_idx[char]] = 1\n",
                "    return one_hot\n",
                "\n",
                "# PrÃ©traitement des images\n",
                "def preprocess_image(image):\n",
                "    img = np.array(image)\n",
                "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
                "    return Image.fromarray(img)\n",
                "\n",
                "# Dataset personnalisÃ©\n",
                "transform = transforms.Compose([\n",
                "    transforms.Grayscale(num_output_channels=1),\n",
                "    transforms.Resize((64, 256)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize((0.5,), (0.5,))\n",
                "])\n",
                "\n",
                "class HandwrittenDataset(Dataset):\n",
                "    def __init__(self, dataframe, transform=None, max_text_len=50):\n",
                "        self.dataframe = dataframe\n",
                "        self.transform = transform\n",
                "        self.max_text_len = max_text_len\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.dataframe)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        img_path = self.dataframe.iloc[idx]['image_path']\n",
                "        text = self.dataframe.iloc[idx]['text']\n",
                "        image = Image.open(img_path).convert('L')\n",
                "        image = preprocess_image(image)\n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "        text_encoded = text_to_one_hot(text, self.max_text_len)\n",
                "        return image, text_encoded, text\n",
                "\n",
                "# CrÃ©er les DataLoaders\n",
                "train_dataset = HandwrittenDataset(train_df, transform=transform)\n",
                "val_dataset = HandwrittenDataset(val_df, transform=transform)\n",
                "test_dataset = HandwrittenDataset(test_df, transform=transform)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
                "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
                "\n",
                "print(f\"Nombre de batches dans train_loader : {len(train_loader)}\")\n",
                "print(f\"Nombre de batches dans val_loader : {len(val_loader)}\")\n",
                "print(f\"Nombre de batches dans test_loader : {len(test_loader)}\")\n",
                "for batch_imgs, batch_texts_encoded, batch_texts in train_loader:\n",
                "    print(f\"Batch images shape : {batch_imgs.shape}\")\n",
                "    print(f\"Batch texts encoded shape : {batch_texts_encoded.shape}\")\n",
                "    print(f\"Exemple de texte : {batch_texts[0]}\")\n",
                "    break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ImplÃ©mentation du cGAN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torchvision.utils import save_image\n",
                "import numpy as np\n",
                "from skimage.metrics import structural_similarity as ssim\n",
                "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
                "\n",
                "# ParamÃ¨tres\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "noise_dim = 100\n",
                "text_dim = 50 * 95  # 50 caractÃ¨res max, vocabulaire de 95\n",
                "img_channels = 1\n",
                "img_size = 64\n",
                "epochs = 200\n",
                "batch_size = 32\n",
                "lr = 0.0002\n",
                "beta1 = 0.5\n",
                "lambda_gp = 10  # Gradient penalty coefficient for WGAN-GP\n",
                "\n",
                "# GÃ©nÃ©rateur\n",
                "class Generator(nn.Module):\n",
                "    def __init__(self, noise_dim, text_dim, img_channels=1, hidden_dim=256):\n",
                "        super(Generator, self).__init__()\n",
                "        self.text_embedding = nn.LSTM(input_size=text_dim, hidden_size=hidden_dim, batch_first=True)\n",
                "        self.main = nn.Sequential(\n",
                "            nn.ConvTranspose2d(noise_dim + hidden_dim, 512, 4, 1, 0, bias=False),\n",
                "            nn.BatchNorm2d(512),\n",
                "            nn.ReLU(True),\n",
                "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
                "            nn.BatchNorm2d(256),\n",
                "            nn.ReLU(True),\n",
                "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
                "            nn.BatchNorm2d(128),\n",
                "            nn.ReLU(True),\n",
                "            nn.ConvTranspose2d(128, img_channels, 4, 2, 1, bias=False),\n",
                "            nn.Tanh()\n",
                "        )\n",
                "\n",
                "    def forward(self, noise, text):\n",
                "        text = text.view(-1, 50, 95)\n",
                "        text_embed, _ = self.text_embedding(text)\n",
                "        text_embed = text_embed[:, -1, :]\n",
                "        text_embed = text_embed.unsqueeze(2).unsqueeze(3)\n",
                "        x = torch.cat([noise, text_embed.expand(-1, -1, 4, 4)], dim=1)\n",
                "        return self.main(x)\n",
                "\n",
                "# Discriminateur\n",
                "class Discriminator(nn.Module):\n",
                "    def __init__(self, img_channels=1, text_dim=95, feature_maps=64):\n",
                "        super(Discriminator, self).__init__()\n",
                "        self.leakyrelu = nn.LeakyReLU(0.2, inplace=True)\n",
                "        self.text_fc = nn.Linear(text_dim, feature_maps * 4)\n",
                "\n",
                "        self.conv = nn.Sequential(\n",
                "            nn.Conv2d(img_channels, feature_maps, 4, 2, 1, bias=False),\n",
                "            nn.LeakyReLU(0.2, inplace=True),\n",
                "            nn.Conv2d(feature_maps, feature_maps * 2, 4, 2, 1, bias=False),\n",
                "            nn.BatchNorm2d(feature_maps * 2),\n",
                "            nn.LeakyReLU(0.2, inplace=True),\n",
                "            nn.Conv2d(feature_maps * 2, feature_maps * 4, 4, 2, 1, bias=False),\n",
                "            nn.BatchNorm2d(feature_maps * 4),\n",
                "            nn.LeakyReLU(0.2, inplace=True),\n",
                "        )\n",
                "\n",
                "        self.output = nn.Conv2d(feature_maps * 8, 1, 4, 1, 0, bias=False)\n",
                "\n",
                "    def forward(self, img, text):\n",
                "        # âœ… Correction du texte\n",
                "        if text.dim() == 3:\n",
                "            text_embed = text.mean(dim=1)\n",
                "        elif text.dim() == 2:\n",
                "            text_embed = text\n",
                "        else:\n",
                "            raise ValueError(f\"Forme inattendue pour le texte : {text.shape}\")\n",
                "\n",
                "        # Projeter le texte\n",
                "        text_feat = self.leakyrelu(self.text_fc(text_embed))\n",
                "\n",
                "        # Extraire les features image\n",
                "        img_feat = self.conv(img)\n",
                "\n",
                "        # Ã‰tendre le texte pour concatÃ©ner\n",
                "        text_feat = text_feat.unsqueeze(2).unsqueeze(3)\n",
                "        text_feat = text_feat.expand(-1, -1, img_feat.size(2), img_feat.size(3))\n",
                "\n",
                "        combined = torch.cat((img_feat, text_feat), dim=1)\n",
                "        validity = self.output(combined)\n",
                "        return validity.view(-1)\n",
                "\n",
                "\n",
                "# Fonction de perte WGAN-GP\n",
                "def gradient_penalty(discriminator, real_img, fake_img, text, device):\n",
                "    batch_size = real_img.size(0)\n",
                "    alpha = torch.rand(batch_size, 1, 1, 1).to(device)\n",
                "    interpolates = alpha * real_img + (1 - alpha) * fake_img\n",
                "    interpolates = interpolates.requires_grad_(True)\n",
                "    d_interpolates = discriminator(interpolates, text)\n",
                "    gradients = torch.autograd.grad(outputs=d_interpolates, inputs=interpolates,\n",
                "                                  grad_outputs=torch.ones_like(d_interpolates),\n",
                "                                  create_graph=True, retain_graph=True)[0]\n",
                "    gradients = gradients.view(batch_size, -1)\n",
                "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
                "    return gradient_penalty\n",
                "\n",
                "# Fonction de calcul des mÃ©triques\n",
                "def calculate_metrics(real_img, fake_img, device):\n",
                "    real_img = real_img.cpu().numpy().transpose(0, 2, 3, 1)\n",
                "    fake_img = fake_img.cpu().numpy().transpose(0, 2, 3, 1)\n",
                "    ssim_score = ssim(real_img[0, ..., 0], fake_img[0, ..., 0], data_range=1.0)\n",
                "    psnr_score = psnr(real_img[0, ..., 0], fake_img[0, ..., 0], data_range=1.0)\n",
                "    return 0, ssim_score, psnr_score  # FID placeholder"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Boucle d'entraÃ®nement"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "ename": "RuntimeError",
                    "evalue": "shape '[-1, 50, 95]' is invalid for input of size 3040",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# ðŸ”¹ 1. EntraÃ®ner le Discriminateur\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     25\u001b[39m optim_d.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m fake_imgs = \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m real_validity = discriminator(real_imgs, text_encoded)\n\u001b[32m     29\u001b[39m fake_validity = discriminator(fake_imgs.detach(), text_encoded)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mGenerator.forward\u001b[39m\u001b[34m(self, noise, text)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, noise, text):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     text = \u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m95\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     text_embed, _ = \u001b[38;5;28mself\u001b[39m.text_embedding(text)\n\u001b[32m     42\u001b[39m     text_embed = text_embed[:, -\u001b[32m1\u001b[39m, :]\n",
                        "\u001b[31mRuntimeError\u001b[39m: shape '[-1, 50, 95]' is invalid for input of size 3040"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import torch\n",
                "from torchvision.utils import save_image\n",
                "\n",
                "os.makedirs(\"generated\", exist_ok=True)\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    for i, (real_imgs, text_encoded, _) in enumerate(train_loader):\n",
                "        batch_size = real_imgs.size(0)\n",
                "        real_imgs = real_imgs.to(device)\n",
                "        text_encoded = text_encoded.to(device)\n",
                "\n",
                "        # ðŸ”§ Ajuster la forme du texte (si besoin)\n",
                "        if text_encoded.dim() > 2:\n",
                "            # Parfois, le DataLoader retourne (batch, seq_len, emb_dim)\n",
                "            # On prend la moyenne sur la sÃ©quence pour avoir (batch, emb_dim)\n",
                "            text_encoded = text_encoded.mean(dim=1)\n",
                "\n",
                "        # ðŸ”§ S'assurer que noise est de la bonne forme\n",
                "        noise = torch.randn(batch_size, noise_dim).to(device)\n",
                "\n",
                "        # ============================================================\n",
                "        # ðŸ”¹ 1. EntraÃ®ner le Discriminateur\n",
                "        # ============================================================\n",
                "        optim_d.zero_grad()\n",
                "        fake_imgs = generator(noise, text_encoded)\n",
                "\n",
                "        real_validity = discriminator(real_imgs, text_encoded)\n",
                "        fake_validity = discriminator(fake_imgs.detach(), text_encoded)\n",
                "\n",
                "        gp = gradient_penalty(discriminator, real_imgs, fake_imgs.detach(), text_encoded, device)\n",
                "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp\n",
                "        d_loss.backward()\n",
                "        optim_d.step()\n",
                "\n",
                "        # ============================================================\n",
                "        # ðŸ”¹ 2. EntraÃ®ner le GÃ©nÃ©rateur\n",
                "        # ============================================================\n",
                "        optim_g.zero_grad()\n",
                "        fake_validity = discriminator(fake_imgs, text_encoded)\n",
                "        g_loss = -torch.mean(fake_validity)\n",
                "        g_loss.backward()\n",
                "        optim_g.step()\n",
                "\n",
                "        # ============================================================\n",
                "        # ðŸ”¹ 3. Affichage et Sauvegarde intermÃ©diaire\n",
                "        # ============================================================\n",
                "        if i % 100 == 0:\n",
                "            with torch.no_grad():\n",
                "                fid, ssim_score, psnr_score = calculate_metrics(real_imgs, fake_imgs, device)\n",
                "                print(f\"[Epoch {epoch+1}/{epochs}] [Batch {i}/{len(train_loader)}] \"\n",
                "                      f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}] \"\n",
                "                      f\"[SSIM: {ssim_score:.4f}] [PSNR: {psnr_score:.4f}]\")\n",
                "\n",
                "                # Sauvegarder un Ã©chantillon dâ€™images\n",
                "                save_image(fake_imgs.data[:25],\n",
                "                           f\"generated/epoch_{epoch+1}_batch_{i}.png\",\n",
                "                           nrow=5, normalize=True)\n",
                "\n",
                "    # ============================================================\n",
                "    # ðŸ”¹ 4. Validation Ã  la fin de chaque Ã©poque\n",
                "    # ============================================================\n",
                "    generator.eval()\n",
                "    val_d_loss = 0\n",
                "    val_g_loss = 0\n",
                "\n",
                "    with torch.no_grad():\n",
                "        for real_imgs, text_encoded, _ in val_loader:\n",
                "            real_imgs = real_imgs.to(device)\n",
                "            text_encoded = text_encoded.to(device)\n",
                "\n",
                "            if text_encoded.dim() > 2:\n",
                "                text_encoded = text_encoded.mean(dim=1)\n",
                "\n",
                "            batch_size = real_imgs.size(0)\n",
                "            noise = torch.randn(batch_size, noise_dim).to(device)\n",
                "            fake_imgs = generator(noise, text_encoded)\n",
                "\n",
                "            real_validity = discriminator(real_imgs, text_encoded)\n",
                "            fake_validity = discriminator(fake_imgs, text_encoded)\n",
                "            gp = gradient_penalty(discriminator, real_imgs, fake_imgs, text_encoded, device)\n",
                "\n",
                "            val_d_loss += (-torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gp).item()\n",
                "            val_g_loss += (-torch.mean(fake_validity)).item()\n",
                "\n",
                "        val_d_loss /= len(val_loader)\n",
                "        val_g_loss /= len(val_loader)\n",
                "\n",
                "        print(f\"âœ… Validation [Epoch {epoch+1}/{epochs}] \"\n",
                "              f\"[D loss: {val_d_loss:.4f}] [G loss: {val_g_loss:.4f}]\")\n",
                "\n",
                "    generator.train()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Ã‰valuation finale sur l'ensemble de test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ã‰valuation finale\n",
                "generator.eval()\n",
                "test_fid, test_ssim, test_psnr = 0, 0, 0\n",
                "test_batches = 0\n",
                "with torch.no_grad():\n",
                "    for real_imgs, text_encoded, _ in test_loader:\n",
                "        real_imgs = real_imgs.to(device)\n",
                "        text_encoded = text_encoded.to(device)\n",
                "        batch_size = real_imgs.size(0)\n",
                "        noise = torch.randn(batch_size, noise_dim, 1, 1).to(device)\n",
                "        fake_imgs = generator(noise, text_encoded)\n",
                "        fid_batch, ssim_batch, psnr_batch = calculate_metrics(real_imgs, fake_imgs, device)\n",
                "        test_fid += fid_batch\n",
                "        test_ssim += ssim_batch\n",
                "        test_psnr += psnr_batch\n",
                "        test_batches += 1\n",
                "test_fid /= test_batches\n",
                "test_ssim /= test_batches\n",
                "test_psnr /= test_batches\n",
                "print(f\"Test Metrics - FID: {test_fid:.4f}, SSIM: {test_ssim:.4f}, PSNR: {test_psnr:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
