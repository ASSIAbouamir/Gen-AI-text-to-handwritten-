{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c52a3a",
   "metadata": {},
   "source": [
    "# üñãÔ∏è Projet Text to Handwriting ‚Äì Notebook complet\n",
    "Ce notebook pr√©pare les donn√©es IAM et entra√Æne un mod√®le simple Text ‚Üí Handwriting avec PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc61be",
   "metadata": {},
   "source": [
    "## üìÇ 1. Importation des biblioth√®ques et v√©rification des dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac2d1d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\user1\\appdata\\roaming\\python\\python313\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\user1\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user1\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user1\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user1\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user1\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5685e9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ IAM dataset pr√™t\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Chemins dataset IAM\n",
    "data_dir = \"IAM_dataset\"\n",
    "lines_file = os.path.join(data_dir, \"ascii\", \"lines.txt\")\n",
    "lines_img_dir = os.path.join(data_dir, \"lines\")\n",
    "\n",
    "# V√©rification\n",
    "assert os.path.exists(data_dir), \"Le dossier IAM_dataset est introuvable\"\n",
    "assert os.path.exists(lines_file), \"Le fichier lines.txt est introuvable\"\n",
    "assert os.path.exists(lines_img_dir), \"Le dossier lines est introuvable\"\n",
    "print(\"‚úÖ IAM dataset pr√™t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da73759",
   "metadata": {},
   "source": [
    "## üßæ 2. Lecture et parsing du fichier lines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72866c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes valides : 11344\n",
      "                                       image_path  \\\n",
      "0  IAM_dataset\\lines\\a01\\a01-000u\\a01-000u-00.png   \n",
      "1  IAM_dataset\\lines\\a01\\a01-000u\\a01-000u-01.png   \n",
      "2  IAM_dataset\\lines\\a01\\a01-000u\\a01-000u-02.png   \n",
      "3  IAM_dataset\\lines\\a01\\a01-000u\\a01-000u-04.png   \n",
      "4  IAM_dataset\\lines\\a01\\a01-000u\\a01-000u-06.png   \n",
      "\n",
      "                                        text  \n",
      "0          A MOVE to stop Mr. Gaitskell from  \n",
      "1      nominating any more Labour life Peers  \n",
      "2       is to be made at a meeting of Labour  \n",
      "3       put down a resolution on the subject  \n",
      "4  Griffiths , M P for Manchester Exchange .  \n"
     ]
    }
   ],
   "source": [
    "# Lecture et parsing\n",
    "lines_data = []\n",
    "with open(lines_file, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if line.startswith('#') or not line.strip():\n",
    "            continue\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) >= 9:\n",
    "            img_id, status = parts[0], parts[1]\n",
    "            if status == 'ok':\n",
    "                text = ' '.join(parts[8:]).replace('|', ' ')\n",
    "                img_base = img_id.split('-')[0]\n",
    "                sub_dir = '-'.join(img_id.split('-')[:2])\n",
    "                img_name = f\"{img_id}.png\"\n",
    "                img_path = os.path.join(lines_img_dir, img_base, sub_dir, img_name)\n",
    "                if os.path.exists(img_path):\n",
    "                    lines_data.append((img_path, text))\n",
    "\n",
    "df_lines = pd.DataFrame(lines_data, columns=['image_path','text'])\n",
    "print(f\"Nombre de lignes valides : {len(df_lines)}\")\n",
    "print(df_lines.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441619cd",
   "metadata": {},
   "source": [
    "## üßπ 3. Nettoyage et split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89289a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 10209, Test: 1135\n"
     ]
    }
   ],
   "source": [
    "# Nettoyage\n",
    "df_lines = df_lines[df_lines['text'].str.strip() != '']\n",
    "df_lines = df_lines[df_lines['text'].str.len() < 100]\n",
    "\n",
    "# Split train/test\n",
    "train_df, test_df = train_test_split(df_lines, test_size=0.1, random_state=42)\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cfa78",
   "metadata": {},
   "source": [
    "## üîÑ 4. Transformations et Dataset PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393680d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfe838c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b4e23ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 79\n"
     ]
    }
   ],
   "source": [
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((32,128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Dataset personnalis√©\n",
    "class HandwritingGenDataset(Dataset):\n",
    "    def __init__(self, dataframe, char2idx, transform=None, max_len=100):\n",
    "        self.dataframe = dataframe\n",
    "        self.char2idx = char2idx\n",
    "        self.transform = transform\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['image_path']\n",
    "        text = self.dataframe.iloc[idx]['text']\n",
    "\n",
    "        # Texte en indices\n",
    "        seq = [self.char2idx[c] for c in text if c in self.char2idx]\n",
    "        if len(seq) < self.max_len:\n",
    "            seq += [0]*(self.max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:self.max_len]\n",
    "        seq = torch.tensor(seq, dtype=torch.long)\n",
    "\n",
    "        # Image\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return seq, image\n",
    "\n",
    "# Vocabulaire\n",
    "all_text = ''.join(df_lines['text'].values)\n",
    "vocab = sorted(list(set(all_text)))\n",
    "vocab_size = len(vocab)\n",
    "char2idx = {c:i for i,c in enumerate(vocab)}\n",
    "idx2char = {i:c for i,c in enumerate(vocab)}\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef9f1c",
   "metadata": {},
   "source": [
    "## üì¶ 5. Cr√©er DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ab77cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch seqs shape: torch.Size([32, 100])\n",
      "Batch images shape: torch.Size([32, 1, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "max_len = 100\n",
    "train_dataset = HandwritingGenDataset(train_df, char2idx, transform=transform, max_len=max_len)\n",
    "test_dataset  = HandwritingGenDataset(test_df, char2idx, transform=transform, max_len=max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# V√©rification\n",
    "for seqs, imgs in train_loader:\n",
    "    print(f\"Batch seqs shape: {seqs.shape}\")\n",
    "    print(f\"Batch images shape: {imgs.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f017c6b7",
   "metadata": {},
   "source": [
    "## üèóÔ∏è 6. D√©finir le mod√®le Text ‚Üí Handwriting simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "274e25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToHandwriting(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128, img_channels=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 32*128)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1,64,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,img_channels,3,padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        h = h.squeeze(0)\n",
    "        h = self.fc(h)\n",
    "        h = h.view(-1,1,32,128)\n",
    "        img = self.conv(h)\n",
    "        return img\n",
    "\n",
    "# Initialiser le mod√®le\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = TextToHandwriting(vocab_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978205c8",
   "metadata": {},
   "source": [
    "## üî• 7. Boucle d'entra√Ænement simplifi√©e (1 epoch test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7329ac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7090\n"
     ]
    }
   ],
   "source": [
    "for seqs, imgs in train_loader:\n",
    "    seqs, imgs = seqs.to(device), imgs.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(seqs)\n",
    "    loss = criterion(outputs, imgs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    break  # tester 1 batch seulement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7be1de67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Loss: 0.0646\n",
      "Epoch 2/50 - Loss: 0.0551\n",
      "Epoch 3/50 - Loss: 0.0549\n",
      "Epoch 4/50 - Loss: 0.0549\n",
      "Epoch 5/50 - Loss: 0.0548\n",
      "Epoch 6/50 - Loss: 0.0555\n",
      "Epoch 7/50 - Loss: 0.0550\n",
      "Epoch 8/50 - Loss: 0.0549\n",
      "Epoch 9/50 - Loss: 0.0548\n",
      "Epoch 10/50 - Loss: 0.0554\n",
      "Epoch 11/50 - Loss: 0.0548\n",
      "Epoch 12/50 - Loss: 0.0548\n",
      "Epoch 13/50 - Loss: 0.0550\n",
      "Epoch 14/50 - Loss: 0.0548\n",
      "Epoch 15/50 - Loss: 0.0550\n",
      "Epoch 16/50 - Loss: 0.0549\n",
      "Epoch 17/50 - Loss: 0.0549\n",
      "Epoch 18/50 - Loss: 0.0548\n",
      "Epoch 19/50 - Loss: 0.0549\n",
      "Epoch 20/50 - Loss: 0.0548\n",
      "Epoch 21/50 - Loss: 0.0548\n",
      "Epoch 22/50 - Loss: 0.0548\n",
      "Epoch 23/50 - Loss: 0.0548\n",
      "Epoch 24/50 - Loss: 0.0558\n",
      "Epoch 25/50 - Loss: 0.0549\n",
      "Epoch 26/50 - Loss: 0.0548\n",
      "Epoch 27/50 - Loss: 0.0548\n",
      "Epoch 28/50 - Loss: 0.0554\n",
      "Epoch 29/50 - Loss: 0.0549\n",
      "Epoch 30/50 - Loss: 0.0548\n",
      "Epoch 31/50 - Loss: 0.0548\n",
      "Epoch 32/50 - Loss: 0.0548\n",
      "Epoch 33/50 - Loss: 0.0551\n",
      "Epoch 34/50 - Loss: 0.0549\n",
      "Epoch 35/50 - Loss: 0.0548\n",
      "Epoch 36/50 - Loss: 0.0548\n",
      "Epoch 37/50 - Loss: 0.0548\n",
      "Epoch 38/50 - Loss: 0.0548\n",
      "Epoch 39/50 - Loss: 0.0550\n",
      "Epoch 40/50 - Loss: 0.0548\n",
      "Epoch 41/50 - Loss: 0.0548\n",
      "Epoch 42/50 - Loss: 0.0547\n",
      "Epoch 43/50 - Loss: 0.0548\n",
      "Epoch 44/50 - Loss: 0.0548\n",
      "Epoch 45/50 - Loss: 0.0548\n",
      "Epoch 46/50 - Loss: 0.0549\n",
      "Epoch 47/50 - Loss: 0.0549\n",
      "Epoch 48/50 - Loss: 0.0548\n",
      "Epoch 49/50 - Loss: 0.0549\n",
      "Epoch 50/50 - Loss: 0.0548\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for seqs, imgs in train_loader:\n",
    "        seqs, imgs = seqs.to(device), imgs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(seqs)\n",
    "        loss = criterion(outputs, imgs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6be638e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le sauvegard√©\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"text_to_handwriting.pth\")\n",
    "print(\"‚úÖ Mod√®le sauvegard√©\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5881c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextToHandwriting(\n",
       "  (embedding): Embedding(79, 64)\n",
       "  (lstm): LSTM(64, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=4096, bias=True)\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"text_to_handwriting.pth\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d31fa192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_text = \"Hello world\"\n",
    "seq = [char2idx[c] for c in test_text if c in char2idx]\n",
    "seq += [0]*(max_len - len(seq))  # padding\n",
    "seq = torch.tensor([seq], dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_img = model(seq).cpu().squeeze().numpy()\n",
    "    pred_img = (pred_img + 1)/2  # re-normaliser [0,1] pour matplotlib\n",
    "    plt.imshow(pred_img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbd698cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ===========================\n",
    "# üß† Nouveau Mod√®le Text ‚Üí Handwriting (v2)\n",
    "# ===========================\n",
    "\n",
    "class TextToHandwritingV2(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1Ô∏è‚É£ Embedding + encodeur s√©quentiel\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # 2Ô∏è‚É£ D√©codeur (projection + g√©n√©rateur)\n",
    "        self.fc = nn.Linear(hidden_dim, 128 * 8 * 32)\n",
    "        \n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 16√ó64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),   # 32√ó128\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 1, 3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        h = h.squeeze(0)\n",
    "        h = self.fc(h)\n",
    "        h = h.view(-1, 128, 8, 32)\n",
    "        out = self.deconv(h)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a172525a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.2325\n",
      "Epoch 2/10 - Loss: 0.1339\n",
      "Epoch 3/10 - Loss: 0.1306\n",
      "Epoch 4/10 - Loss: 0.1300\n",
      "Epoch 5/10 - Loss: 0.1291\n",
      "Epoch 6/10 - Loss: 0.1288\n",
      "Epoch 7/10 - Loss: 0.1287\n",
      "Epoch 8/10 - Loss: 0.1286\n",
      "Epoch 9/10 - Loss: 0.1286\n",
      "Epoch 10/10 - Loss: 0.1286\n"
     ]
    }
   ],
   "source": [
    "# Initialisation\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = TextToHandwritingV2(vocab_size).to(device)\n",
    "\n",
    "# Fonction de perte : L1 (plus stable pour g√©n√©ration d'images)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ===========================\n",
    "# üî• Entra√Ænement simplifi√© (test)\n",
    "# ===========================\n",
    "num_epochs = 10  # tu peux augmenter √† 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for seqs, imgs in train_loader:\n",
    "        seqs, imgs = seqs.to(device), imgs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(seqs)\n",
    "        loss = criterion(outputs, imgs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Sauvegarde du mod√®le\n",
    "torch.save(model.state_dict(), \"text2handwriting_v2.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "100b36d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_handwriting_v2(text, model, char2idx, device='cpu', max_len=100):\n",
    "    model.eval()\n",
    "    seq = [char2idx[c] for c in text if c in char2idx]\n",
    "    if len(seq) < max_len:\n",
    "        seq += [0]*(max_len - len(seq))\n",
    "    else:\n",
    "        seq = seq[:max_len]\n",
    "    seq = torch.tensor([seq], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(seq).cpu().squeeze().numpy()\n",
    "    pred = (pred + 1) / 2  # re-normalisation [0,1]\n",
    "    plt.imshow(pred, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'\"{text}\"')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2468f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_handwriting_v2(\"Hello world\", model, char2idx, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e4e6f",
   "metadata": {},
   "source": [
    "model.load_state_dict(torch.load(\"text_to_handwriting.pth\"))\n",
    "model.eval()\n",
    "‚úÖ Ce notebook contient toutes les √©tapes :\n",
    "1. Pr√©paration des donn√©es IAM\n",
    "2. Parsing et DataFrame\n",
    "3. Split train/test\n",
    "4. Dataset PyTorch + transformations\n",
    "5. DataLoaders\n",
    "6. Mod√®le Text ‚Üí Handwriting\n",
    "7. Boucle d'entra√Ænement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
